{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e886f31-3b9d-4eab-9af8-954e50c0efc0",
   "metadata": {},
   "source": [
    "# STEPS TO BUILD MACHINE LEARNING MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713d8c30-2110-416c-89e7-4910786db8a1",
   "metadata": {},
   "source": [
    "**I. Data Access and Collection**\n",
    "\n",
    "The initial step in a machine learning problem is to access the necessary data. Typically, data scientists obtain the data for the business problems they are working on by querying the databases where their companies store their data. However, there are also unstructured datasets that don't fit well into a relational database, such as logs, raw texts, images, videos, and more. To process these datasets, data engineers and data scientists use Extract, Transform, Load (ETL) pipelines. These datasets can either be stored in a data lake or a database, either relational or not.\n",
    "\n",
    "If data scientists don't have the data they need to solve their problems, they have several options for obtaining it. They can scrape data from websites, purchase data from data providers, or collect the data from surveys, clickstream data, sensors, cameras, and other sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c048b0c1-f171-428a-8b86-d5a2114a50c0",
   "metadata": {},
   "source": [
    "**II. Data preparation and exploration**\n",
    "\n",
    "Once data has been obtained, the next step for data scientists is to prepare it for analysis by performing various tasks such as data exploration, visualization, transformation, and cleansing. Data preparation involves cleaning and processing raw data to ensure it is accurate and complete. Data scientists need to understand the available data before building any machine learning model. Raw data can often be inaccurate, incomplete, or duplicated, and data scientists must identify and replace or delete these problematic data points.\n",
    "\n",
    "Data scientists also need to determine whether the data has labels or not. For example, if data consists of images and the goal is to develop a detection model to identify cars, data scientists need a set of labeled images that indicate whether there is a car in the image and bounding boxes around the cars. If the images lack labels, data scientists will have to label them, either manually or with the help of labeling tools or vendors.\n",
    "\n",
    "Once the data has been cleansed, data scientists explore the features or variables in their dataset, identifying relationships and transformations between them. They can use various open-source libraries and analytics/data science platforms for exploratory data analysis, which can include statistical analysis and data visualization to generate plots of the features. Data scientists must determine what kind of features the data has, whether they are numerical, categorical, or ordinal, and obtain a distribution of values that each feature has.\n",
    "\n",
    "In the data exploration step, data scientists plot the features and also plot the features against each other to identify patterns in the dataset. This helps to determine the need for data transformation, such as handling missing values, outliers, and correlated features. They must decide how to handle missing values, such as filling them in with the mean, median, or mode, or the nearby entries' values. They also need to decide how to handle outliers and whether they need to normalize the dataset or perform other transformations to rescale the data. Finally, they need to address the long tail of categorical values, deciding whether to use them as-is, group them in a meaningful way, or ignore a subset of them altogether.\n",
    "\n",
    "\n",
    "During the process of exploring a dataset, patterns can be identified that can inspire the creation of new features to better represent the data. This process is known as feature engineering. For example, in a traffic dataset that records the number of vehicles passing through a major intersection at every hour, a new feature could be created that categorizes the hour into different parts of the day, such as early morning, mid-morning, early afternoon, late afternoon, and nighttime.\n",
    "\n",
    "For categorical features, it is often necessary to perform one hot encoding. One hot encoding involves converting a categorical feature into binary features, with one feature for each category. For instance, if a dataset contains information about customers and their state of origin (e.g. Washington, Oregon, or California), one hot encoding would result in two binary features: one indicating whether a customer is from Washington state or not, and another indicating whether a customer is from Oregon or not. Since it can be assumed that customers who are not from Washington or Oregon are from California, there is no need for a third feature to represent California."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3664c-d789-49ef-99a6-23d6317ffb5d",
   "metadata": {},
   "source": [
    "**III. Model build and train**\n",
    "\n",
    "The process of building a machine learning model involves selecting the appropriate model and features to solve the problem. There are two main types of machine learning models: supervised and unsupervised. Supervised learning involves input data with a label, while unsupervised learning involves input data without a label. Data scientists will try multiple models and algorithms and generate multiple model candidates to find the best fit for the dataset.\n",
    "\n",
    "During model training, the dataset is split into training and testing sets. The training dataset is used to train the model, while the testing dataset is used to evaluate the performance of the model on unseen data. Hyperparameter tuning is also an important task in the model training process, where data scientists tune the hyperparameters of a model to improve its performance.\n",
    "\n",
    "Data scientists also need to consider the compute resources required for model training. Depending on the size of the dataset and the complexity of the model, local computing resources may not be enough, and cloud computing resources with specialized hardware such as GPUs may be necessary. Distributed training environments can also speed up the process, especially when dealing with large amounts of data or training multiple model candidates in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eff7a9-daa5-4d7c-98d4-83129632ecd2",
   "metadata": {},
   "source": [
    "**IV. Model Evaluation**\n",
    "\n",
    "Data scientists have access to various open-source tools that can help them calculate and visualize metrics for evaluating machine learning models. These tools can include confusion matrices, ROC curves, AUC-ROC curves, gain and lift charts, and more. However, it's important for data scientists to choose the appropriate evaluation metrics based on the specific business problem they are trying to solve.\n",
    "\n",
    "For example, in a classification problem where the goal is to detect a rare illness, accuracy might not be the most useful metric. Instead, a better metric would be the number of true positives divided by all the people with the illness. This information can be obtained from a confusion matrix, which shows the number of true positives, true negatives, false positives, and false negatives, allowing for the calculation of precision and recall.\n",
    "\n",
    "In regression problems, metrics such as root-mean-square error, mean absolute error, and the coefficient of determination (r2) can be used. For unsupervised problems, the ideal set of clusters would have high cohesion within each cluster and separation between clusters. Metrics such as the silhouette score and Calinski-Harabasz coefficient can be used to measure these qualities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7651b-7cb6-436a-8c51-cf3b8b981099",
   "metadata": {},
   "source": [
    "**V. Model Deployment**\n",
    "\n",
    "After the model training and evaluation processes are complete, the best candidate models are saved in formats such as Pickle, ONNX, and PMML. Data scientists may work on machine learning problems for proof of concept, experimentation, or to deploy the model to production. Model deployment involves consuming the predictions made by the machine learning model in some way, and often the pipeline of data transformations also needs to be deployed. Typically, data scientists work with engineers on model deployment.\n",
    "\n",
    "Depending on how predictions will be consumed, they can be deployed for batch consumption or real-time consumption. For batch consumption, predictions can be scheduled to run at certain intervals (e.g., every hour or every day). The predictions can then be stored in a database and consumed by other applications. Typically, the amount of data processed is larger than for real-time prediction. A use case would be an e-commerce site sending a weekly email to customers about recommended products based on past purchases. Machine learning models can be scheduled to run ahead of time.\n",
    "\n",
    "For real-time consumption, a trigger would initiate the process of using the persisted model to serve a prediction. For example, deciding whether a transaction is fraudulent when payment is initiated requires real-time prediction. The speed of serving the predictions (in milliseconds or seconds), the volume of demand for the service, and the size of data to run predictions on need to be considered. Minimizing latency to serve prediction is important. One can improve serving latency by using a smaller model in size, using accelerators such as GPU, and improving how features related to the entity are retrieved for real-time prediction (e.g., by improving how information on past purchases of the user is fetched when recommending products to a user as they browse a site.)\n",
    "\n",
    "There are different tools and cloud platform offerings for model deployment, such as Functions-as-a-Service (FaaS) platforms, fully managed deployment of models as HTTP endpoints, DIY with Flask or Django in a container orchestration platform such as k8 and Docker Swarm, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267f524-1932-4e9a-ada6-e4179993c6cd",
   "metadata": {},
   "source": [
    "**VI. Model Monitoring**\n",
    "\n",
    "\n",
    "Model monitoring is an important but often overlooked step in machine learning and data science projects. It helps teams decide when to retrain and redeploy models based on changes in data distribution or other factors. There are two components to model monitoring: drift/statistical monitoring and ops monitoring.\n",
    "\n",
    "Drift/statistical monitoring involves comparing the distribution and statistics of the training data with that of live data. This helps identify changes in data that could affect model performance. For example, if a customer churn model is deployed, one could compare the features of customers in the training data with those in the production system. This can also help track the percentage of customers predicted to churn in the training sample compared to live production.\n",
    "\n",
    "Ops monitoring involves tracking the performance and reliability of the machine learning system. This includes monitoring serving latency, memory/CPU usage, throughput, and system reliability. Logs and metrics need to be set up to track and monitor these factors. Logs contain records of events and can be used to investigate specific incidents. Kibana is an open-source tool used for searching and viewing logs. Metrics measure the usage and behavior of the machine learning system. Prometheus and Grafana are tools for monitoring metrics.\n",
    "\n",
    "Both drift/statistical monitoring and ops monitoring are important for ensuring that machine learning models are performing as intended in a production environment. By regularly monitoring models, teams can identify when it is necessary to retrain and redeploy them, improving the overall effectiveness of the machine learning system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea0f5c-608b-469c-95d6-aab5929da774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
